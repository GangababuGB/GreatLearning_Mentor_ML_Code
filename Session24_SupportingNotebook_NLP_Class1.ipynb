{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SupportingNotebook_NLP_Class1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2vgbrF0zbCU"
      },
      "source": [
        "# Example Sentences <br>\n",
        "\n",
        "I saw the Grand Canyon flying to New York <br>\n",
        "Lost child found by the house <br>\n",
        "Why are you speaking to him as a child ? <br>\n",
        "\n",
        "Computer Slow <br>\n",
        "Clean Memory <br>\n",
        "Memory full <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzbZ6lsDdFhp"
      },
      "source": [
        "# Tokenization\n",
        "text = \"We are learning Natural Language Processing today\""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0H7048GdGGK",
        "outputId": "c2a012e8-ea82-4f95-b9c2-194a4f1dfedc"
      },
      "source": [
        "for i,w in enumerate(text.split()):\n",
        "  print(\"Token\" + str(i) + \":\" + w)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token0:We\n",
            "Token1:are\n",
            "Token2:learning\n",
            "Token3:Natural\n",
            "Token4:Language\n",
            "Token5:Processing\n",
            "Token6:today\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaQ124u8dGaS",
        "outputId": "2f8662ae-3df1-4283-aa2d-2a3073242df7"
      },
      "source": [
        "# Vocab Function if reqd to create\n",
        "from collections import Counter\n",
        "corpus = [('This is a not really good movie',1),    0\n",
        "          ('This movie is just bad',1),  \n",
        "          ('I would like to see this movie again',1),\n",
        "          ('How was this movie even released ?',1),\n",
        "          ]\n",
        "C = Counter()\n",
        "\n",
        "for review,label in corpus:\n",
        "  C.update(review.split())\n",
        "print(C)\n",
        "C.most_common(3)\n",
        "\n",
        "            movie  this really good just  not_really\n",
        "TF            1      1     1\n",
        "IDF           4~0      2     1~4\n",
        "TFIDF          0      1     4"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'movie': 4, 'This': 2, 'is': 2, 'this': 2, 'a': 1, 'really': 1, 'good': 1, 'just': 1, 'bad': 1, 'I': 1, 'would': 1, 'like': 1, 'to': 1, 'see': 1, 'again': 1, 'How': 1, 'was': 1, 'even': 1, 'released': 1, '?': 1})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('movie', 4), ('This', 2), ('is', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IolQ9QrvHct"
      },
      "source": [
        "## Compare NLTK Vs Spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGpx3r-eutV9"
      },
      "source": [
        "text = \"We are learning Natural Language Processing today\""
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJrThpMYu92O",
        "outputId": "7ad15cbc-8bb9-4ee9-8703-2c988d968071"
      },
      "source": [
        "# nltk implementation\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xia3MYFhu9sd"
      },
      "source": [
        "#spaCy Code Initialization:\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text_spacy = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAnh8SKzumW1",
        "outputId": "a3495fdb-3c9c-4400-fcff-691c936fcacf"
      },
      "source": [
        "nltk_tokenList = word_tokenize(text)\n",
        "nltk_tokenList"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We', 'are', 'learning', 'Natural', 'Language', 'Processing', 'today']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4c4A8JvvSb9",
        "outputId": "e943c688-cb54-44cb-d28b-08d753310e89"
      },
      "source": [
        "#spaCy\n",
        "token_text = []\n",
        "for token in text_spacy:\n",
        "    token_text.append(token.text)\n",
        "print(token_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We', 'are', 'learning', 'Natural', 'Language', 'Processing', 'today']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U4QD-FJdGiS",
        "outputId": "2f798c15-e850-437d-f217-0f913b97b343"
      },
      "source": [
        "# Stemming\n",
        "text = \"We are studying Natural Language Processing today.\"\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "Stemmer = SnowballStemmer(language='english')\n",
        "for token in text.split(\" \"):\n",
        "  print(token,'->',Stemmer.stem(token))\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We -> we\n",
            "are -> are\n",
            "studying -> studi\n",
            "Natural -> natur\n",
            "Language -> languag\n",
            "Processing -> process\n",
            "today. -> today.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLJs6Fhqv74u"
      },
      "source": [
        "# Spacy provides a function to do both Stemming & Lemmantization together"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1bIxGEydGpC",
        "outputId": "00e5ec92-4a8f-4445-9ff7-a2412e486268"
      },
      "source": [
        "# Lemmatization\n",
        "Lemmatizer = WordNetLemmatizer()\n",
        "'''\n",
        "for token in text.split(\" \"):\n",
        "  print(token,'->',Lemmatizer.lemmatize(token))\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "lemma_nltk = []\n",
        "for word, tag in pos_tag(word_tokenize(text)):\n",
        "     wntag = tag[0].lower()\n",
        "     if wntag not in ['a', 'n', 'v']:  # Noun, Verb, Adjective, \n",
        "             lemma_nltk.append(word)\n",
        "     else:\n",
        "             lemma_nltk.append(Lemmatizer.lemmatize(word, wntag))\n",
        "     \n",
        "print(lemma_nltk)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We', 'be', 'learn', 'Natural', 'Language', 'Processing', 'today']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkjE6RLJ51V7",
        "outputId": "44967879-c154-4315-8d42-e89fa54f7d02"
      },
      "source": [
        "pos_tag(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('studying', 'VBG'),\n",
              " ('Natural', 'NNP'),\n",
              " ('Language', 'NNP'),\n",
              " ('Processing', 'NNP'),\n",
              " ('today', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au06OvFywZlX",
        "outputId": "125b00c8-304a-4361-d9d8-f2a3a168e827"
      },
      "source": [
        "#spaCy\n",
        "lemma_text = []\n",
        "for token in text_spacy:\n",
        "    lemma_text.append(token.lemma_)\n",
        "print(\"Tokenize+Lemmatize:\")\n",
        "print(lemma_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize+Lemmatize:\n",
            "['-PRON-', 'be', 'learn', 'Natural', 'Language', 'Processing', 'today']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhOBbVB0dGwR",
        "outputId": "717d623d-75ee-44a9-a966-f867a6f8da47"
      },
      "source": [
        "# Remove Stopwords\n",
        "#NLTK\n",
        "print(lemma_nltk)\n",
        "filtered_sentence_nltk = []  \n",
        "nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "for w in lemma_nltk:  \n",
        "    if w not in nltk_stop_words:  \n",
        "        filtered_sentence_nltk.append(w)\n",
        "\n",
        "print(filtered_sentence_nltk)      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We', 'be', 'study', 'Natural', 'Language', 'Processing', 'today', '.']\n",
            "['We', 'study', 'Natural', 'Language', 'Processing', 'today', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyzRN3CZ3WP0",
        "outputId": "5369ef71-8634-4780-ed15-132f6005e466"
      },
      "source": [
        "#Spacey\n",
        "print(lemma_nltk)\n",
        "filtered_sentence_spacy =[] \n",
        "for word in lemma_nltk:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence_spacy.append(word) \n",
        "print(filtered_sentence_spacy)       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We', 'be', 'study', 'Natural', 'Language', 'Processing', 'today', '.']\n",
            "['study', 'Natural', 'Language', 'Processing', 'today', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPsjFVYK4eB7",
        "outputId": "fcd8077e-d76a-4078-b96b-61e04da5f241"
      },
      "source": [
        "# Remove Punctuation - Same in NLTP & Spacy\n",
        "list_nltk = ['We', 'study', 'Natural', 'Language', 'Processing', 'today', '.']\n",
        "print(list_nltk)\n",
        "punctuations=\"?:!.,;\"\n",
        "for word in list_nltk:\n",
        "      if word in punctuations:\n",
        "          print(word)\n",
        "          list_nltk.remove(word)\n",
        "\n",
        "print(list_nltk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We', 'study', 'Natural', 'Language', 'Processing', 'today', '.']\n",
            ".\n",
            "['We', 'study', 'Natural', 'Language', 'Processing', 'today']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwgzIcJvzR3k"
      },
      "source": [
        "# Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANE_qUp6ybdO"
      },
      "source": [
        "World -> Mental Models -> Decisions <br>\n",
        "Data -> Mathematical Models -> Predictions\n",
        "\n",
        "P(The weather is nice today) > P(The whether is nice today) <br>\n",
        "\n",
        "2 ways of approaching problem - <br>\n",
        "Statistical Way <br>\n",
        "Neural Net Way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ8Z6JbSzJZ0"
      },
      "source": [
        "Example - <br>\n",
        "We are learning Natural Language Processing Today. <br>\n",
        "model = {('we','are'):{'learning':0.5},{'reading':0.5}} <br>\n",
        "\n",
        "P(W1,W2,...Wm) <br>= i=1 to m π(P(Wi | W1...Wi-1)) <br>≈ i=1 to m π(P(Wi | Wi-(n-1)...Wi-1)) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6xxg10OY_vZ",
        "outputId": "53cb70ba-8529-4acd-f13d-1248e52ad82e"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from zipfile import ZipFile\n",
        "nltk.download('reuters')\n",
        "zf = ZipFile('/root/nltk_data/corpora/reuters.zip', 'r')\n",
        "zf.extractall('/root/nltk_data/corpora')\n",
        "zf.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP_h0d0ulTQE",
        "outputId": "48140bed-9b89-4f8f-bfd4-854360eda9d1"
      },
      "source": [
        "# quick summary of the reuters corpus\n",
        "print(\">>> The reuters corpus has {} tags\".format(len(reuters.categories())))\n",
        "print(\">>> The reuters corpus has {} documents\".format(len(reuters.fileids())))\n",
        "#News excerpts with 90 topic labels \n",
        "reuters.categories()[:5]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> The reuters corpus has 90 tags\n",
            ">>> The reuters corpus has 10788 documents\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['acq', 'alum', 'barley', 'bop', 'carcass']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrBnWUKLYjbr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGTKiEamj95u"
      },
      "source": [
        "from nltk import bigrams, trigrams\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "model = defaultdict(lambda: defaultdict(lambda:0))\n",
        "\n",
        "for sentence in reuters.sents(): # Get individual words from sentence\n",
        "  for word1,word2,word3 in trigrams(sentence,pad_right=True,pad_left=True):\n",
        "    model[(word1,word2)][word3] += 1   # Get unique combinations of trigrams for all words in reuters\n",
        "\n",
        "for word1_word2 in model:\n",
        "  total_count = float(sum(model[word1_word2].values())) #Normalize the counts between 0 and 1\n",
        "  for word3 in model[word1_word2]:\n",
        "    model[word1_word2][word3] /= total_count"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjvILy8tj-BO",
        "outputId": "b3c3c353-21ac-442a-f6ec-710450a347a2"
      },
      "source": [
        "{k: v for k,v in sorted(dict(model[\"the\",\"stock\"]).items(),key=lambda item: item[0],reverse = True)}\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\"': 0.0051813471502590676,\n",
              " \"'\": 0.0051813471502590676,\n",
              " ',': 0.046632124352331605,\n",
              " '.': 0.046632124352331605,\n",
              " '.\"': 0.0051813471502590676,\n",
              " 'Wednesday': 0.0051813471502590676,\n",
              " 'adjustment': 0.0051813471502590676,\n",
              " 'after': 0.0051813471502590676,\n",
              " 'and': 0.015544041450777202,\n",
              " 'as': 0.02072538860103627,\n",
              " 'at': 0.02072538860103627,\n",
              " 'being': 0.0051813471502590676,\n",
              " 'between': 0.0051813471502590676,\n",
              " 'closed': 0.0051813471502590676,\n",
              " 'collapse': 0.0051813471502590676,\n",
              " 'depreciation': 0.0051813471502590676,\n",
              " 'dividend': 0.025906735751295335,\n",
              " 'engine': 0.0051813471502590676,\n",
              " 'exchange': 0.02072538860103627,\n",
              " 'exchanges': 0.0051813471502590676,\n",
              " 'fell': 0.0051813471502590676,\n",
              " 'focusing': 0.0051813471502590676,\n",
              " 'for': 0.08808290155440414,\n",
              " 'from': 0.010362694300518135,\n",
              " 'goes': 0.0051813471502590676,\n",
              " 'going': 0.0051813471502590676,\n",
              " 'grew': 0.0051813471502590676,\n",
              " 'has': 0.010362694300518135,\n",
              " 'he': 0.0051813471502590676,\n",
              " 'in': 0.031088082901554404,\n",
              " 'increased': 0.0051813471502590676,\n",
              " 'into': 0.0051813471502590676,\n",
              " 'is': 0.03626943005181347,\n",
              " 'jumped': 0.0051813471502590676,\n",
              " 'languished': 0.0051813471502590676,\n",
              " 'last': 0.0051813471502590676,\n",
              " 'manager': 0.0051813471502590676,\n",
              " 'market': 0.08290155440414508,\n",
              " 'may': 0.010362694300518135,\n",
              " 'moved': 0.0051813471502590676,\n",
              " 'nearly': 0.0051813471502590676,\n",
              " 'of': 0.10880829015544041,\n",
              " 'offering': 0.0051813471502590676,\n",
              " 'on': 0.02072538860103627,\n",
              " 'position': 0.0051813471502590676,\n",
              " 'price': 0.025906735751295335,\n",
              " 'purchase': 0.010362694300518135,\n",
              " 'purchases': 0.015544041450777202,\n",
              " 'recently': 0.0051813471502590676,\n",
              " 'represents': 0.0051813471502590676,\n",
              " 'repurchase': 0.0051813471502590676,\n",
              " 'rose': 0.0051813471502590676,\n",
              " 'sales': 0.0051813471502590676,\n",
              " 'since': 0.0051813471502590676,\n",
              " 'split': 0.05181347150259067,\n",
              " 'subscription': 0.0051813471502590676,\n",
              " 'through': 0.0051813471502590676,\n",
              " 'to': 0.04145077720207254,\n",
              " 'today': 0.0051813471502590676,\n",
              " 'total': 0.0051813471502590676,\n",
              " 'totaling': 0.0051813471502590676,\n",
              " 'under': 0.0051813471502590676,\n",
              " 'was': 0.025906735751295335,\n",
              " 'will': 0.015544041450777202,\n",
              " 'would': 0.0051813471502590676}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaHqCRl6j-hO",
        "outputId": "b730c0e0-9f8b-42ee-857d-5aad764af552"
      },
      "source": [
        "import random\n",
        "\n",
        "#starting words\n",
        "text = [\"today\",\"the\",\"president\"]\n",
        "sentence_finished = False\n",
        "\n",
        "while not sentence_finished:\n",
        "  r = random.random() #generate random #s\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in model[tuple(text[-2:])].keys():  # Take last 2 characters to get probabilities\n",
        "    accumulator += model[tuple(text[-2:])][word]  # Get the high probability word from model greater than random probability\n",
        "    #select words above probability threshold\n",
        "    if accumulator >= r:\n",
        "      text.append(word)\n",
        "      break\n",
        "\n",
        "  if text[-2:] == [None,None]:\n",
        "    sentence_finished = True\n",
        "\n",
        "print(' '.join([t for t in text if t]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "today the president of administration John Guminski said in a statement .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpiGUtNkj-rl",
        "outputId": "a8c74adf-a552-4e4d-ed3e-8726679a6319"
      },
      "source": [
        "import random\n",
        "\n",
        "#starting words\n",
        "text = [\"today\",\"the\"]\n",
        "sentence_finished = False\n",
        "\n",
        "while not sentence_finished:\n",
        "  r = random.random()\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in model[tuple(text[-2:])].keys():\n",
        "    accumulator += model[tuple(text[-2:])][word]\n",
        "    #select words above probability threshold\n",
        "    if accumulator >= r:\n",
        "      text.append(word)\n",
        "      break\n",
        "\n",
        "  if text[-2:] == [None,None]:\n",
        "    sentence_finished = True\n",
        "\n",
        "print(' '.join([t for t in text if t]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "today the company ' s 1987 gross domestic product risks not reaching the top of a golden period ,\" said Pesch .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zm3-dW5pGFg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}